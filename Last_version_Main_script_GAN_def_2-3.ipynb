{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ungRrsq7Fz8I"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install monai\n",
        "!pip install matplotlib\n",
        "!pip install nibabel\n",
        "import datetime\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import monai\n",
        "from PIL import Image\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    AddChanneld,\n",
        "    LoadImage,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    Rand3DElasticd,\n",
        "    RandAffined,\n",
        "    Spacingd,\n",
        ")\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjAWkUOvFg-4"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./Last_version_Main_script_GAN_def.ipynb\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E5qSf9ZGO-J"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "data_path = r'/content/drive/My drive/Deep Learning/ACDC/training'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0iy525Ffvi"
      },
      "outputs": [],
      "source": [
        "def get_dict():\n",
        "  data_path = r'/content/drive/My Drive/Deep Learning/ACDC/training'\n",
        "\n",
        "  patient_images = glob.glob(os.path.join(data_path,'*'))\n",
        "  dicts = []\n",
        "  for images_patients in patient_images:\n",
        "        # find the binary mask that belongs to the original image, based on indexing in the filename\n",
        "        mask_path = glob.glob(os.path.join(images_patients,'*gt.nii.gz'))\n",
        "        data = glob.glob(os.path.join(images_patients,'*'))\n",
        "        for idx in data:\n",
        "          image_index = os.path.split(idx)[1].split('_')[-1].split('.')[0]\n",
        "          patient_number = os.path.split(idx)[1].split('_')\n",
        "          if image_index != 'gt' and image_index != '4d' and image_index != 'Info':\n",
        "                 img = os.path.join(f'{images_patients}/{patient_number[0]}_{image_index}.nii.gz')\n",
        "                 mask = os.path.join(f'{images_patients}/{patient_number[0]}_{image_index}_gt.nii.gz')\n",
        "                 #time_dim = os.path.join(f'{images_patients}/{patient_number[0]}_4d.nii.gz')\n",
        "                 if os.path.exists(mask) and os.path.exists(img):\n",
        "                    dicts.append({'image':img,'mask':mask,'patient_info':f'{patient_number[0]}_{image_index}'})\n",
        "  return dicts\n",
        "\n",
        "\n",
        "        \n",
        "          \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5_XIGcGqXWD"
      },
      "outputs": [],
      "source": [
        "total_list = get_dict()\n",
        "val_list = total_list[0:20]\n",
        "train_list = total_list[19:-1]\n",
        "test_train_list = total_list[0:2]\n",
        "print(test_train_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxZb9DeFv1UL"
      },
      "outputs": [],
      "source": [
        " \n",
        "train_dataset = monai.data.Dataset(train_list)\n",
        "val_dataset = monai.data.Dataset(val_list)\n",
        "loader = LoadImaged(keys=(\"image\", \"mask\"))\n",
        "\n",
        "data_dict = loader(train_dataset[1])\n",
        "# print(f\"input:, {train_data_dicts[0]}\")\n",
        "print(f\"image shape: {data_dict['image'].shape}\")\n",
        "print(f\"label shape: {data_dict['mask'].shape}\")\n",
        "print(f\"image pixdim:\\n{data_dict['image_meta_dict']['pixdim']}\")\n",
        "\n",
        "image, mask = data_dict[\"image\"], data_dict[\"mask\"]\n",
        "plt.figure(\"visualize\", (8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image[:, :, 5])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"mask\")\n",
        "plt.imshow(mask[:, :, 5])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j7cxDUZ3vw6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoXG7G923C_3"
      },
      "outputs": [],
      "source": [
        "from matplotlib import transforms\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# constructDataset from list of paths + transform\n",
        "transform = monai.transforms.Compose(\n",
        "[\n",
        "    LoadImaged(keys=(\"image\", \"mask\")),\n",
        "    monai.transforms.AddChanneld(keys=[\"image\",\"mask\"]),\n",
        "    monai.transforms.CenterSpatialCropd(keys =[\"image\",\"mask\"], roi_size= [128,128,-1]),\n",
        "    monai.transforms.Spacingd(keys= [\"image\",\"mask\"],pixdim=(1,1,1),meta_key_postfix = 'image_meta_dict', mode = ['nearest ','nearest']),\n",
        "    monai.transforms.AdjustContrastd(keys = [\"image\"], gamma=(0.5)),\n",
        "    monai.transforms.HistogramNormalized(keys =[\"image\"],num_bins=256, min=0, max=255),\n",
        "    monai.transforms.AsDiscreted(keys =[\"mask\"], to_onehot=4),\n",
        "    monai.transforms.AsChannelFirstd(keys = [\"image\",\"mask\"], channel_dim=- 1),\n",
        "    monai.transforms.ToTensord(keys=[\"image\",\"mask\"])\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dot8tozeHUl6"
      },
      "outputs": [],
      "source": [
        "\n",
        "val_dataset = monai.data.CacheDataset(val_list, transform=transform)\n",
        "train_dataset = monai.data.CacheDataset(train_list, transform=transform)\n",
        "test_train_dataset =monai.data.CacheDataset(test_train_list, transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from monai.utils import first\n",
        "validation_loader = monai.data.DataLoader(val_dataset,)\n",
        "train_loader = monai.data.DataLoader(train_dataset)\n",
        "test_train_loader = monai.data.DataLoader(test_train_dataset,batch_size =1,num_workers=2)\n",
        "\n",
        "print(first(validation_loader)[\"patient_info\"])\n"
      ],
      "metadata": {
        "id": "URwk_RQb1QoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGWBxcWY16oQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "data = first(train_loader)\n",
        "mask = np.argmax(data[\"mask\"][0,2,:,:,:],0)\n",
        "\n",
        "\n",
        "plt.imshow(mask)\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(data[\"image\"][0,2,0,:,:])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX8ryl4gE5lA"
      },
      "outputs": [],
      "source": [
        "def from_compose_to_list(transform_compose):\n",
        "    \"\"\"\n",
        "    Transform an object monai.transforms.Compose in a list fully describing the transform.\n",
        "    /!\\ Random seed is not saved, then reproducibility is not enabled.\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "        \n",
        "    if not isinstance(transform_compose, monai.transforms.Compose):\n",
        "        raise TypeError(\"transform_compose should be a monai.transforms.Compose object.\")\n",
        "    \n",
        "    output_list = list()\n",
        "    for transform in transform_compose.transforms:\n",
        "        kwargs = deepcopy(vars(transform))\n",
        "        \n",
        "        # Remove attributes which are not arguments\n",
        "        args = list(transform.__init__.__code__.co_varnames[1: transform.__init__.__code__.co_argcount])\n",
        "        for key, obj in vars(transform).items():\n",
        "            if key not in args:\n",
        "                del kwargs[key]\n",
        "\n",
        "        output_list.append({\"class\": transform.__class__, \"kwargs\": kwargs})\n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yryVLbM--eG"
      },
      "outputs": [],
      "source": [
        "torch.cuda.get_device_name(torch.cuda.current_device())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'The used device is {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkLbbO8ECj5c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Unet(device):\n",
        "  model = nn.Sequential(monai.networks.nets.UNet(\n",
        "      spatial_dims=2,\n",
        "      in_channels=1,\n",
        "      out_channels=4,\n",
        "      channels=(16, 32, 64, 128,256),\n",
        "      strides=(2, 2,2,2)),torch.nn.Softmax(dim =1)\n",
        "  ).to(device)\n",
        "  return model"
      ],
      "metadata": {
        "id": "c7BCzeQ2jUJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-de0XaYHC6Q-"
      },
      "outputs": [],
      "source": [
        "loss_function =monai.losses.DiceLoss(softmax=False, batch = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaNQKcuIDxkJ"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZKfmzWUa2oWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnSv94yCEoBN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "model = Unet(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "run = wandb.init(\n",
        "    project='ACDC_segmentation_unet2',\n",
        "    name='run_at_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    config={\n",
        "        'loss function': str(loss_function), \n",
        "        'lr': optimizer.param_groups[0][\"lr\"],\n",
        "        'transform': from_compose_to_list(transform),\n",
        "        'batch_size': test_train_loader.batch_size,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n",
        "# For example you should add information on your model...\n",
        "\n",
        "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n",
        "\n",
        "def log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs):\n",
        "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n",
        "\n",
        "    # Create list of images that have segmentation masks for model output and ground truth\n",
        "    #log_imgs = [wandb.Image(img, masks=wandb_masks(mask_output, mask_gt)) for img, mask_output,\n",
        "     #           mask_gt in zip(batch_data['image'], outputs, batch_data['mask'])]\n",
        "\n",
        "    # Send epoch, losses and images to W&B\n",
        "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
        "    \n",
        "# TO BE REMOVED\n",
        "trigger_times = 0\n",
        "last_loss =100\n",
        "patience =4\n",
        "for epoch in tqdm(range(200)):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    \n",
        "    for batch_data in train_loader: \n",
        "        step += 1\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data[\"image\"][0].float().to(device))\n",
        "        loss = loss_function(outputs, batch_data[\"mask\"][0].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    train_loss = epoch_loss/step\n",
        "    # validation part\n",
        "    step = 0\n",
        "    val_loss = 0\n",
        "    for batch_data in validation_loader:\n",
        "        step += 1\n",
        "        model.eval()\n",
        "        outputs = model(batch_data['image'][0].float().to(device))\n",
        "        loss = loss_function(outputs, batch_data['mask'][0].to(device))\n",
        "        val_loss+= loss.item()\n",
        "    val_loss = val_loss / step   \n",
        "    log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs)\n",
        "    current_loss = val_loss\n",
        "    if current_loss > last_loss: \n",
        "            trigger_times += 1\n",
        "            print('Trigger Times:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break\n",
        "  \n",
        "    else:\n",
        "      print('trigger times: 0')\n",
        "      trigger_times = 0\n",
        "      last_loss = current_loss\n",
        "\n",
        "# Store the network parameters        \n",
        "torch.save(model.state_dict(), r'/content/drive/My Drive/Deep Learning/ACDC/trained_Unet_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.pt')\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RZUh3w9kQvJ"
      },
      "outputs": [],
      "source": [
        "import numbers\n",
        "model = Unet(device)\n",
        "model.load_state_dict(torch.load(r'/content/drive/My Drive/Deep Learning/ACDC/trained_Unet_20220620-073124.pt'))\n",
        "\n",
        "output = model(first(validation_loader)['image'][0].to(device))\n",
        "output = np.argmax(output.detach().cpu().numpy(),1)\n",
        "\n",
        "num = 4\n",
        "print(output.shape)\n",
        "plt.figure(figsize=(12.80,7.20))\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(output[num])\n",
        "plt.colorbar()\n",
        "plt.subplot(2,1,2)\n",
        "plt.imshow(first(validation_loader)['image'][0][num].squeeze(),cmap ='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWICIlNNxO9-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Discriminator_CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Discriminator_CNN, self).__init__()\n",
        "        self.layers = nn.Sequential(nn.Conv2d(in_channels= 5, out_channels=64, kernel_size=5, stride=2, padding=2),\n",
        "                                    nn.LeakyReLU(0.2),\n",
        "                                    nn.Dropout(0.3),\n",
        "                                    nn.BatchNorm2d(num_features = 64),\n",
        "                                    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),\n",
        "                                    nn.LeakyReLU(0.2),\n",
        "                                    nn.Dropout(0.3),\n",
        "                                    nn.BatchNorm2d(num_features = 128),\n",
        "                                    nn.Flatten(),\n",
        "                                    nn.Linear(in_features=131072, out_features=1),\n",
        "                                    nn.Sigmoid())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def AddNoise(data):\n",
        "    gaussian = monai.transforms.RandGaussianNoise(prob =1,mean =0.5, std = 0.1)\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    data = gaussian(data)\n",
        "    data = torch.clamp(data, min= 0,max =1)\n",
        "    data =softmax(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "etLlArS5eDFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch,generator, device, valid_loader, loss_function,train_loss,gen_loss,dis_loss,dis2_loss):\n",
        "\n",
        "    generator.eval()\n",
        "    loss_total = 0\n",
        "\n",
        "    # Test validation data\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "\n",
        "            outputs = generator(data['image'][0].float().to(device))\n",
        "            val_loss = loss_function(outputs, data['mask'][0].to(device))\n",
        "            loss_total += val_loss.item()\n",
        "        val_loss = loss_total / len(valid_loader) \n",
        "        log_to_wandb(epoch, train_loss, val_loss,data,gen_loss,dis_loss,dis2_loss)\n",
        "\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "wAcsM-tSJUFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = first(train_loader)\n",
        "mask_data = data['mask'][0]\n",
        "data_noise = AddNoise(mask_data)\n",
        "arg_data = np.argmax(data_noise.detach().cpu().numpy(),1)\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(data_noise[5,1,:,:])\n",
        "plt.colorbar()\n",
        "plt.subplot(2,1,2)\n",
        "plt.imshow(arg_data[5,:,:])\n"
      ],
      "metadata": {
        "id": "coKAu081-AgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tqeqabhbgeEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-PMNR3TlEFM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import wandb\n",
        " \n",
        "\n",
        "\n",
        "def log_to_wandb(epoch, train_loss, val_loss, batch_data,gen_loss,dis_loss,dis2_loss):\n",
        "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n",
        "\n",
        "    # Create list of images that have segmentation masks for model output and ground truth\n",
        "    #log_imgs = [wandb.Image(img, masks=wandb_masks(mask_output, mask_gt)) for img, mask_output,\n",
        "     #           mask_gt in zip(batch_data['image'], outputs, batch_data['mask'])]\n",
        "\n",
        "    # Send epoch, losses and images to W&B\n",
        "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss,'dis_loss': dis_loss,'dis_loss2':dis2_loss,'gen_loss':gen_loss})\n",
        "\n",
        "def train_gan(epochs,lr_gen,lr_dis,b1,b2,patience,devide_epoch,gen_epoch ,weight_gen,weight_dis,device,train_loader,validation_loader):\n",
        "  # Get networks\n",
        "  discriminator = Discriminator_CNN().to(device)\n",
        "  generator = Unet(device)\n",
        "\n",
        "  loss_dis= nn.BCELoss()\n",
        "  loss_dice = monai.losses.DiceLoss(softmax= False , batch=True)\n",
        "  # Configure optimizers and loss function\n",
        "  optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=lr_dis, betas=(b1, b2))\n",
        "  optimizer_gen = torch.optim.Adam(generator.parameters(), lr=lr_gen, betas=(b1, b2))\n",
        "\n",
        "  run = wandb.init(\n",
        "    project='ACDC_segmentation_def',\n",
        "    name='run_at_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    config={\n",
        "        'loss function dis': str(loss_dis), \n",
        "        'loss function gen': str(loss_dice), \n",
        "        'lr_dis': optimizer_dis.param_groups[0][\"lr\"],\n",
        "        'lr_gen': optimizer_gen.param_groups[0][\"lr\"],\n",
        "        'b1':b1,\n",
        "        'b2': b2,\n",
        "        'Weight_gen': weight_gen,\n",
        "        'Weight_dis': weight_dis,\n",
        "        'devide_epoch': devide_epoch,\n",
        "        \"generrator epoch\":gen_epoch,\n",
        "        'transform': from_compose_to_list(transform),\n",
        "        'batch_size': train_loader.batch_size,\n",
        "    }\n",
        ")\n",
        "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n",
        "# For example you should add information on your model...\n",
        "\n",
        "  run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n",
        "\n",
        "  last_loss = 100\n",
        "  triggertimes = 0\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    # Wrap dataloader into tqdm such that we can print progress while training\n",
        "\n",
        "        generator.train()\n",
        "\n",
        "        epoch_GAN_loss = 0\n",
        "        epoch_gen_loss = 0\n",
        "        epoch_dis2_loss = 0\n",
        "        epoch_dis_loss = 0\n",
        "        step_dis = 0\n",
        "        step_gen = 0\n",
        "        loop_2 = 0\n",
        "        dis_loss = 0\n",
        "        for batch_data in train_loader:\n",
        "            \n",
        "            loop_2 += 1\n",
        "            if epoch > gen_epoch and loop_2 % devide_epoch > 0:\n",
        "              step_dis +=1\n",
        "              for param in generator.parameters():\n",
        "                  param.requires_grad = False\n",
        "              for param in discriminator.parameters():\n",
        "                  param.requires_grad = True\n",
        "              discriminator.zero_grad()\n",
        "\n",
        "               # Generate some fake MNIST images using the generator\n",
        "              fake_images = generator(batch_data[\"image\"][0].float().to(device))\n",
        "              input_images = batch_data[\"image\"][0].float().to(device)\n",
        "              target_images = batch_data[\"mask\"][0].to(device)\n",
        "              target_images_with_noise = AddNoise(target_images)\n",
        "            \n",
        "\n",
        "            # Concatenate the fake and real images\n",
        "              \n",
        "              dis_input_1 = torch.cat((fake_images, input_images), dim= 1)\n",
        "              dis_input_2 = torch.cat((target_images_with_noise, input_images),dim= 1)\n",
        "              dis_input_def = torch.cat((dis_input_1,dis_input_2),dim=0)\n",
        "\n",
        "\n",
        "              predictions = discriminator(dis_input_def)\n",
        "              dis_labels_0 = torch.zeros((batch_data[\"image\"][0].shape[0], 1), device=device)\n",
        "              dis_labels_1 = torch.ones((batch_data[\"image\"][0].shape[0], 1), device=device)\n",
        "              dis_labels_def = torch.cat((dis_labels_0,dis_labels_1),dim=0)\n",
        "              \n",
        "              \n",
        "              \n",
        "              dis_loss_batch = loss_dis(predictions, dis_labels_def)\n",
        "              \n",
        "                \n",
        "              dis_loss_batch.backward()\n",
        "              optimizer_dis.step()\n",
        "              epoch_dis_loss += dis_loss_batch\n",
        "            if epoch < gen_epoch or loop_2 % devide_epoch == 0:\n",
        "              step_gen +=1\n",
        "              for param in generator.parameters():\n",
        "                param.requires_grad = True\n",
        "              for param in discriminator.parameters():\n",
        "                param.requires_grad = False\n",
        "              generator.zero_grad()\n",
        "              fake_images = generator(batch_data[\"image\"][0].float().to(device))\n",
        "              input_images = batch_data[\"image\"][0].float().to(device)\n",
        "              target_images = batch_data[\"mask\"][0].to(device)\n",
        "              dis_input = torch.cat((fake_images,input_images),dim=1)\n",
        "              predictions = discriminator(dis_input)\n",
        "              gen_labels = torch.zeros((batch_data[\"image\"][0].shape[0], 1), device=device)\n",
        "              dis_loss_2 = loss_dis(predictions,gen_labels)\n",
        "              gen_loss = loss_dice(target_images,fake_images)\n",
        "              if epoch < gen_epoch:\n",
        "                GAN_loss = weight_gen*gen_loss\n",
        "              else:\n",
        "                GAN_loss = (weight_gen*gen_loss)+(weight_dis*dis_loss_2)\n",
        "\n",
        "\n",
        "              GAN_loss.backward()\n",
        "              optimizer_gen.step()\n",
        "              epoch_GAN_loss += GAN_loss.item()\n",
        "              epoch_gen_loss += gen_loss.item()\n",
        "              epoch_dis2_loss += dis_loss_2.item()\n",
        "\n",
        "        train_loss = epoch_GAN_loss/step_gen\n",
        "        gen_loss = epoch_gen_loss/step_gen\n",
        "        if step_dis > 0:\n",
        "          dis_loss = epoch_dis_loss/step_dis\n",
        "        else:\n",
        "          dis_loss = 0\n",
        "        dis2_loss = epoch_dis2_loss/step_gen\n",
        "          \n",
        "\n",
        "        current_loss = validation(epoch,generator, device, validation_loader, loss_dice,train_loss,gen_loss,dis_loss,dis2_loss)\n",
        "        print('The Current Loss:', current_loss)\n",
        "\n",
        "        if current_loss > last_loss and epoch> gen_epoch+10:\n",
        "            trigger_times += 1\n",
        "            print('Trigger Times:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            print('trigger times: 0')\n",
        "            trigger_times = 0\n",
        "            \n",
        "            last_loss = current_loss\n",
        "\n",
        "       \n",
        "        \n",
        "              \n",
        "           \n",
        "\n",
        "            #torch.autograd.set_detect_anomaly(True)\n",
        "            \n",
        "            \n",
        "            # Train generator with a new batch of generated samples\n",
        "            \n",
        "            \n",
        "            # From the generator's perspective, the discriminator should predict\n",
        "            # ones for all samples\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # Train the GAN to predict ones\n",
        "            \n",
        "              \n",
        "        \n",
        "        \n",
        "\n",
        "  torch.save(generator.state_dict(), r'/content/drive/My Drive/Deep Learning/ACDC/trainedGAN_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.pt')\n",
        "  run.finish()\n",
        "  return generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loader = monai.data.DataLoader(val_dataset, batch_size=6)\n",
        "train_loader = monai.data.DataLoader(train_dataset, batch_size=6)\n",
        "test_train_loader = monai.data.DataLoader(test_train_dataset, batch_size=2)\n"
      ],
      "metadata": {
        "id": "w746ggLYzmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = train_gan(epochs = 1000,lr_gen = 0.002,lr_dis = 0.00005,b1 = 0.5,b2 = 0.999,\n",
        "                      patience =10,devide_epoch =2,gen_epoch =20,weight_gen =5,weight_dis =1,device = device,train_loader = train_loader,validation_loader = validation_loader)"
      ],
      "metadata": {
        "id": "joLLC8aPVJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pSqFD9H5836"
      },
      "outputs": [],
      "source": [
        "def Unet(device):\n",
        "  model = nn.Sequential(monai.networks.nets.UNet(\n",
        "      spatial_dims=2,\n",
        "      in_channels=1,\n",
        "      out_channels=4,\n",
        "      channels=(16, 32, 64, 128,256),\n",
        "      strides=(2, 2,2,2)),torch.nn.Softmax(dim=1)\n",
        "  ).to(device)\n",
        "  return model\n",
        "\n",
        "\n",
        "GAN = Unet(device)\n",
        "GAN.load_state_dict(torch.load(r'/content/drive/My Drive/Deep Learning/ACDC/trainedGAN_20220620-074121.pt'))\n",
        "GAN.eval()\n",
        "output_GAN = GAN(first(validation_loader)['image'][0].to(device))\n",
        "output_GAN = np.argmax(output_GAN.detach().cpu().numpy(),1)\n",
        "\n",
        "\n",
        "Unet = Unet(device)\n",
        "Unet.load_state_dict(torch.load(r'/content/drive/My Drive/Deep Learning/ACDC/trained_Unet_20220620-073124.pt'))\n",
        "\n",
        "output_unet = Unet(first(validation_loader)['image'][0].to(device))\n",
        "output_unet = np.argmax(output_unet.detach().cpu().numpy(),1)\n",
        "\n",
        "\n",
        "num = 2\n",
        "mask_gt = np.argmax(first(validation_loader)['mask'][0,num],0)\n",
        "\n",
        "plt.figure(figsize=(12.80,7.20))\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(output_GAN[num])\n",
        "plt.title('Segmentatie Unet+GAN')\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(output_unet[num])\n",
        "plt.title('Segmentatie Unet')\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(mask_gt)\n",
        "plt.title('Segmentatie Groundtruth')\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(first(validation_loader)['image'][0][num].squeeze(),cmap ='gray')\n",
        "plt.title('MRI Scan')\n",
        "plt.tight_layout()\n",
        "plt.savefig(r'/content/drive/My Drive/Deep Learning/ACDC/Table_1.png', dpi = 600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cluh3LL6Mti"
      },
      "outputs": [],
      "source": [
        "def get_dict_test():\n",
        "  data_path = r'/content/drive/My Drive/Deep Learning/ACDC/testing'\n",
        "\n",
        "  patient_images = glob.glob(os.path.join(data_path,'*'))\n",
        "  dicts = []\n",
        "  for images_patients in patient_images:\n",
        "        # find the binary mask that belongs to the original image, based on indexing in the filename\n",
        "        mask_path = glob.glob(os.path.join(images_patients,'*gt.nii.gz'))\n",
        "        data = glob.glob(os.path.join(images_patients,'*'))\n",
        "        for idx in data:\n",
        "          image_index = os.path.split(idx)[1].split('_')[-1].split('.')[0]\n",
        "          patient_number = os.path.split(idx)[1].split('_')\n",
        "          if image_index != 'gt' and image_index != '4d' and image_index and 'Info':\n",
        "                 img = os.path.join(f'{images_patients}/{patient_number[0]}_{image_index}.nii.gz')\n",
        "                 #time_dim = os.path.join(f'{images_patients}/{patient_number[0]}_4d.nii.gz')\n",
        "                 if os.path.exists(img):\n",
        "                    dicts.append({'image':img, 'dict':f'{patient_number[0]}/{patient_number[0]}_{image_index}'})\n",
        "  return dicts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = get_dict_test()\n",
        "print(test_list)\n",
        "transform_test = monai.transforms.Compose(\n",
        "[\n",
        "    LoadImaged(keys=(\"image\")),\n",
        "    monai.transforms.AddChanneld(keys=[\"image\"]),\n",
        "    monai.transforms.CenterSpatialCropd(keys =[\"image\"], roi_size= [128,128,-1]),\n",
        "    monai.transforms.Spacingd(keys= [\"image\"],pixdim=(1,1,1),meta_key_postfix = 'image_meta_dict', mode =['nearest']),\n",
        "    monai.transforms.AdjustContrastd(keys = [\"image\"], gamma=(0.5)),\n",
        "    monai.transforms.HistogramNormalized(keys =[\"image\"],num_bins=256, min=0, max=255),\n",
        "    monai.transforms.AsChannelFirstd(keys = [\"image\"], channel_dim=- 1),\n",
        "    monai.transforms.ToTensord(keys=[\"image\"])\n",
        "    ])\n",
        "\n",
        "test_dataset = monai.data.CacheDataset(test_list, transform=transform_test)\n",
        "test_loader = monai.data.DataLoader(test_dataset, batch_size =1, num_workers = 2)\n",
        "\n",
        "trans_inv = monai.transforms.Compose(\n",
        "[\n",
        "    monai.transforms.CenterSpatialCropd(keys =[\"mask\"], roi_size= [128,128,-1]),\n",
        "    monai.transforms.Spacingd(keys= [\"image\"],pixdim=(1,1,1),meta_key_postfix = 'image_meta_dict', mode =['nearest']),\n",
        "    monai.transforms.ToTensord(keys=[\"image\"])\n",
        "    ])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_-JaVHYgswp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " t = monai.transforms.ToTensord(keys=[\"image\"])\n",
        " print(isinstance(t,monai.transforms.InvertibleTransform))"
      ],
      "metadata": {
        "id": "R-JFAiMaOzXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_nii(img_path, data, affine, header):\n",
        "    \"\"\"\n",
        "    Function to save a 'nii' or 'nii.gz' file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    img_path: string\n",
        "    Path to save the image should be ending with '.nii' or '.nii.gz'.\n",
        "\n",
        "    data: np.array\n",
        "    Numpy array of the image data.\n",
        "\n",
        "    affine: list of list or np.array\n",
        "    The affine transformation to save with the image.\n",
        "\n",
        "    header: nib.Nifti1Header\n",
        "    The header that define everything about the data\n",
        "    (pleasecheck nibabel documentation).\n",
        "    \"\"\"\n",
        "    nimg = nib.Nifti1Image(data,affine =affine, header=header)\n",
        "    nimg.to_filename(img_path)"
      ],
      "metadata": {
        "id": "WzZpLKQzg2sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Unet(device):\n",
        "  model = nn.Sequential(monai.networks.nets.UNet(\n",
        "      spatial_dims=2,\n",
        "      in_channels=1,\n",
        "      out_channels=4,\n",
        "      channels=(16, 32, 64, 128,256),\n",
        "      strides=(2, 2,2,2)),torch.nn.Softmax(dim=1)\n",
        "  ).to(device)\n",
        "  return model\n",
        "\n",
        "\n",
        "model = Unet(device)\n",
        "model.load_state_dict(torch.load(r'/content/drive/My Drive/Deep Learning/ACDC/trainedGAN_20220620-074121.pt'))\n",
        "\n",
        "path = r'/content/drive/My Drive/Deep Learning/ACDC/results/'\n",
        "for patient in test_loader:\n",
        "    output = model(patient[\"image\"][0].float().to(device))\n",
        "    output = np.argmax(output.detach().cpu().numpy(),1)\n",
        "    patient_dict = patient[\"dict\"]\n",
        "    patient_number = os.path.split(patient_dict[0])[1].split('_')[0]\n",
        "    patient_frame = os.path.split(patient_dict[0])[1].split('_')[-1]\n",
        "\n",
        "    if patient_frame == 'frame01':\n",
        "      save_nii(path+'ED/'+str(patient_number)+'_ED.nii.gz',output,None,None)\n",
        "    else:\n",
        "      save_nii(path+'ES/'+str(patient_number)+'_ES.nii.gz',output,None,None)\n",
        "\n",
        "    \n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gvBny4tzKLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_nii(img_path):\n",
        "    \"\"\"\n",
        "    Function to load a 'nii' or 'nii.gz' file, The function returns\n",
        "    everyting needed to save another 'nii' or 'nii.gz'\n",
        "    in the same dimensional space, i.e. the affine matrix and the header\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    img_path: string\n",
        "    String with the path of the 'nii' or 'nii.gz' image file name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Three element, the first is a numpy array of the image values,\n",
        "    the second is the affine transformation of the image, and the\n",
        "    last one is the header of the image.\n",
        "    \"\"\"\n",
        "    nimg = nib.load(img_path)\n",
        "    return nimg.get_data(), nimg.affine, nimg.header"
      ],
      "metadata": {
        "id": "XroTYV3RCKR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, affine, header =load_nii(r'/content/drive/My Drive/Deep Learning/ACDC/results/ED/patient150_ED.nii.gz')\n",
        "data = trans_inv.inverse(data)\n",
        "plt.imshow(data[5])"
      ],
      "metadata": {
        "id": "4UvJbqYqCZoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "def Unet(device):\n",
        "  model = nn.Sequential(monai.networks.nets.UNet(\n",
        "      spatial_dims=2,\n",
        "      in_channels=1,\n",
        "      out_channels=4,\n",
        "      channels=(16, 32, 64, 128,256),\n",
        "      strides=(2, 2,2,2)),torch.nn.Softmax(dim=1)\n",
        "  ).to(device)\n",
        "  return model\n",
        "\n",
        "loss_dice = monai.losses.DiceLoss(softmax= False , batch=True)\n",
        "model = Unet(device)\n",
        "model.load_state_dict(torch.load(r'/content/drive/My Drive/Deep Learning/ACDC/trained_Unet_20220620-073124.pt'))\n",
        "patient_list = []\n",
        "path = r'/content/drive/My Drive/Deep Learning/ACDC/results/'\n",
        "count = 0\n",
        "for patient in validation_loader:\n",
        "    count+=1\n",
        "    output = np.argmax(model(patient[\"image\"][0].float().to(device)).detach().cpu().numpy(),1)\n",
        "    ground_truth = np.argmax(patient[\"mask\"][0].detach().cpu().numpy(),1)\n",
        "    patient_info = patient['patient_info']\n",
        "    patient_number = os.path.split(patient_info[0])[1].split('_')[0]\n",
        "    patient_frame = os.path.split(patient_info[0])[1].split('_')[-1]\n",
        "    print(patient_frame)\n",
        "    if patient_frame == 'frame01':\n",
        "      patient_info = str(patient_number)+'_ED'\n",
        "    else:\n",
        "      patient_info = str(patient_number)+'_ES'\n",
        "    res = metrics(ground_truth,output,[1,1,1])\n",
        "    res = np.reshape(res,(1,9))\n",
        "    if count == 1:\n",
        "      pd1 = pd.DataFrame(data=res,columns = (\"Dice LV\", \"Volume LV\", \"Err LV(ml)\",\n",
        "    \"Dice RV\",\"Volume RV\",\" Err RV(ml)\", \"Dice MYO\", \"Volume MYO\", \"Err MYO(ml)\"),index = [patient_info])\n",
        "    else:\n",
        "      pd2 = pd.DataFrame(data=res,columns =(\"Dice LV\", \"Volume LV\", \"Err LV(ml)\",\n",
        "    \"Dice RV\",\"Volume RV\",\" Err RV(ml)\", \"Dice MYO\", \"Volume MYO\", \"Err MYO(ml)\"), index = [patient_info])\n",
        "      pd1 = pd1.append(pd2)\n",
        "      \n",
        "\n",
        "\n",
        "writer = pd.ExcelWriter(r'/content/drive/My Drive/Deep Learning/ACDC/data.xlsx',egine='xlsxwriter') \n",
        "pd1.to_excel(writer,header =True)\n",
        "writer.save()\n"
      ],
      "metadata": {
        "id": "Ig0MOfUSf5mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(img_gt, img_pred, voxel_size):\n",
        "    \"\"\"\n",
        "    Function to compute the metrics between two segmentation maps given as input.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_gt: np.array\n",
        "    Array of the ground truth segmentation map.\n",
        "\n",
        "    img_pred: np.array\n",
        "    Array of the predicted segmentation map.\n",
        "\n",
        "    voxel_size: list, tuple or np.array\n",
        "    The size of a voxel of the images used to compute the volumes.\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    A list of metrics in this order, [Dice LV, Volume LV, Err LV(ml),\n",
        "    Dice RV, Volume RV, Err RV(ml), Dice MYO, Volume MYO, Err MYO(ml)]\n",
        "    \"\"\"\n",
        "\n",
        "    if img_gt.ndim != img_pred.ndim:\n",
        "        raise ValueError(\"The arrays 'img_gt' and 'img_pred' should have the \"\n",
        "                         \"same dimension, {} against {}\".format(img_gt.ndim,\n",
        "                                                                img_pred.ndim))\n",
        "\n",
        "    res = []\n",
        "    # Loop on each classes of the input images\n",
        "    for c in [3, 1, 2]:\n",
        "        # Copy the gt image to not alterate the input\n",
        "        gt_c_i = np.copy(img_gt)\n",
        "        gt_c_i[gt_c_i != c] = 0\n",
        "\n",
        "        # Copy the pred image to not alterate the input\n",
        "        pred_c_i = np.copy(img_pred)\n",
        "        pred_c_i[pred_c_i != c] = 0\n",
        "\n",
        "        # Clip the value to compute the volumes\n",
        "        gt_c_i = np.clip(gt_c_i, 0, 1)\n",
        "        pred_c_i = np.clip(pred_c_i, 0, 1)\n",
        "\n",
        "        # Compute the Dice\n",
        "        dice = 1-loss_dice(torch.tensor(gt_c_i), torch.tensor(pred_c_i)).detach()\n",
        "\n",
        "        # Compute volume\n",
        "        volpred = pred_c_i.sum() * np.prod(voxel_size) / 1000.\n",
        "        volgt = gt_c_i.sum() * np.prod(voxel_size) / 1000.\n",
        "\n",
        "        res += [dice, volpred, volpred-volgt]\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "IxUKElCw7Weo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(pd1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u5CUmpbuqa-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BExSdaAYxebP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Last_version_Main_script_GAN_def_2.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}